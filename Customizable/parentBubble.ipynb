{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2975e841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusions détectées :\n"
     ]
    }
   ],
   "source": [
    "# Librairies\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Dossier ou sont sauvegarde les donnee apres le modele\n",
    "dataFolder = r\"C:\\Users\\faraboli\\Desktop\\BubbleID\\BubbleIDGit\\ProjetBubbleID\\My_output\\SaveData2\"\n",
    "extension = \"Test6\"\n",
    "contourFile = dataFolder + \"/contours_\" + extension +\".json\"\n",
    "richFile = dataFolder + \"/rich_\" + extension +\".csv\"\n",
    "\n",
    "\n",
    "def mask_from_contour(contour, shape):\n",
    "    mask = np.zeros(shape[:2], dtype=np.uint8)\n",
    "    contour = np.array(contour, dtype=np.int32)\n",
    "    cv2.fillPoly(mask, [contour], 255)\n",
    "    return mask\n",
    "\n",
    "def load_data(json_path, csv_path, shape):\n",
    "    # Charger JSON\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # Charger CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Associer chaque bulle à son track_id et masque\n",
    "    data_by_frame = defaultdict(dict)\n",
    "    for key, contour in json_data.items():\n",
    "        image_id, bubble_id = map(int, key.split('_'))\n",
    "        row = df[(df['frame'] == image_id) & (df['det_in_frame'] == bubble_id)]\n",
    "        if len(row) == 0:\n",
    "            continue\n",
    "        track_id = int(row.iloc[0]['track_id'])\n",
    "        mask = mask_from_contour(contour, shape)\n",
    "        data_by_frame[image_id][track_id] = mask\n",
    "    return data_by_frame\n",
    "\n",
    "def compute_fusions(data_by_frame, threshold=0.01):\n",
    "    fusions = {}\n",
    "    sorted_frames = sorted(data_by_frame.keys())\n",
    "    for i in range(len(sorted_frames) - 1):\n",
    "        prev_frame = sorted_frames[i]\n",
    "        next_frame = sorted_frames[i + 1]\n",
    "        bubbles_prev = data_by_frame[prev_frame]\n",
    "        bubbles_next = data_by_frame[next_frame]\n",
    "\n",
    "        for id_next, mask_next in bubbles_next.items():\n",
    "            intersections = []\n",
    "            for id_prev, mask_prev in bubbles_prev.items():\n",
    "                inter = np.logical_and(mask_prev, mask_next)\n",
    "                inter_area = np.sum(inter)\n",
    "                prev_area = np.sum(mask_prev)\n",
    "                if prev_area == 0:\n",
    "                    continue\n",
    "                overlap = inter_area / prev_area\n",
    "                if overlap > threshold:\n",
    "                    intersections.append(id_prev)\n",
    "            if len(intersections) == 2:\n",
    "                fusions[id_next] = intersections\n",
    "    return fusions\n",
    "\n",
    "\n",
    "#################################################################\n",
    "shape = (1024, 1024)  # Taille de l’image\n",
    "data_by_frame = load_data(contourFile, richFile, shape)\n",
    "fusions = compute_fusions(data_by_frame)\n",
    "print(\"Fusions détectées :\")\n",
    "for new_id, parents in fusions.items():\n",
    "    print(f\"Bulle {new_id} vient de la fusion de {parents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd3c0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusions détectées :\n",
      "Frame 8 : child 20 ← parents [6, 9]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------\n",
    "# PARAMÈTRES\n",
    "# ------------------------\n",
    "IMAGE_SHAPE = (1024, 1024)\n",
    "W_PREV = 3\n",
    "W_NEXT = 3\n",
    "DILATE_ITERS = 1\n",
    "KERNEL = np.ones((3, 3), np.uint8)\n",
    "OVERLAP_THRESH = 0.1\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# UTILITAIRES\n",
    "# ------------------------\n",
    "def mask_from_contour(contour, shape):\n",
    "    mask = np.zeros(shape, dtype=np.uint8)\n",
    "    if len(contour) == 0:\n",
    "        return mask\n",
    "    pts = np.array(contour, dtype=np.int32)\n",
    "    cv2.fillPoly(mask, [pts], 255)\n",
    "    if DILATE_ITERS > 0:\n",
    "        mask = cv2.dilate(mask, KERNEL, iterations=DILATE_ITERS)\n",
    "    return mask\n",
    "\n",
    "def overlap_ratio(mask1, mask2):\n",
    "    inter = np.logical_and(mask1 > 0, mask2 > 0)\n",
    "    area1 = np.sum(mask1 > 0)\n",
    "    return np.sum(inter) / area1 if area1 > 0 else 0.0\n",
    "\n",
    "# ------------------------\n",
    "# CHARGEMENT DES DONNÉES\n",
    "# ------------------------\n",
    "def load_json_contours(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    parsed = {}\n",
    "    for key, contour in data.items():\n",
    "        frame_str, det_str = key.split('_')\n",
    "        frame = int(frame_str)\n",
    "        det_in_frame = int(det_str)\n",
    "        parsed[(frame, det_in_frame)] = contour\n",
    "    return parsed\n",
    "\n",
    "def build_masks_and_index(json_path, csv_path, image_shape=IMAGE_SHAPE):\n",
    "    contours = load_json_contours(json_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    data_by_frame = defaultdict(dict)\n",
    "    for (frame, det_in_frame), contour in contours.items():\n",
    "        row = df[(df['frame'] == frame) & (df['det_in_frame'] == det_in_frame)]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        track_id = int(row.iloc[0]['track_id'])\n",
    "        mask = mask_from_contour(contour, image_shape)\n",
    "        if np.sum(mask > 0) == 0:\n",
    "            continue\n",
    "        data_by_frame[frame][track_id] = mask\n",
    "    return data_by_frame\n",
    "\n",
    "# ------------------------\n",
    "# FONCTION PRINCIPALE\n",
    "# ------------------------\n",
    "def detect_fusions(json_path, csv_path, image_shape=IMAGE_SHAPE):\n",
    "    data_by_frame = build_masks_and_index(json_path, csv_path, image_shape)\n",
    "    fusion_map = {}  # key = new_track_id, value = {'parents': [...], 'frame': t}\n",
    "\n",
    "    frames = sorted(data_by_frame.keys())\n",
    "    for i, frame in enumerate(frames):\n",
    "        current_bubbles = data_by_frame[frame]\n",
    "        if i<len(frames)-1:\n",
    "            next_bubbles = data_by_frame[frame+1]\n",
    "        prev_frames = [f for f in frames if f < frame and f >= frame - W_PREV]\n",
    "\n",
    "        previous_bubbles = {}\n",
    "        for f in prev_frames:\n",
    "            previous_bubbles.update(data_by_frame[f])\n",
    "\n",
    "        new_bubbles = [tid for tid in current_bubbles if tid not in previous_bubbles]\n",
    "        disappeared_bubbles = [tid for tid in previous_bubbles if (tid not in current_bubbles or tid not in next_bubbles)]\n",
    "\n",
    "        for new_tid in new_bubbles:\n",
    "            aggregated_mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "            for f_next in frames:\n",
    "                if f_next < frame:\n",
    "                    continue\n",
    "                if f_next > frame + W_NEXT:\n",
    "                    break\n",
    "                if new_tid in data_by_frame[f_next]:\n",
    "                    aggregated_mask = np.logical_or(aggregated_mask, data_by_frame[f_next][new_tid] > 0)\n",
    "            aggregated_mask = (aggregated_mask.astype(np.uint8) * 255)\n",
    "\n",
    "            parents = []\n",
    "            for parent_tid in disappeared_bubbles:\n",
    "                parent_mask = previous_bubbles[parent_tid]\n",
    "                ratio = overlap_ratio(parent_mask, aggregated_mask)\n",
    "                if ratio > OVERLAP_THRESH:\n",
    "                    parents.append(parent_tid)\n",
    "\n",
    "            if parents:\n",
    "                fusion_map[new_tid] = {'parents': parents, 'frame': frame}\n",
    "\n",
    "    return fusion_map\n",
    "\n",
    "# ------------------------\n",
    "# EXÉCUTION\n",
    "# ------------------------\n",
    "# Dossier ou sont sauvegarde les donnee apres le modele\n",
    "dataFolder = r\"C:\\Users\\faraboli\\Desktop\\BubbleID\\BubbleIDGit\\ProjetBubbleID\\My_output\\SaveData2\"\n",
    "extension = \"Test6\"\n",
    "contourFile = dataFolder + \"/contours_\" + extension +\".json\"\n",
    "richFile = dataFolder + \"/rich_\" + extension +\".csv\"\n",
    "\n",
    "fusions = detect_fusions(contourFile, richFile, IMAGE_SHAPE)\n",
    "print(\"Fusions détectées :\")\n",
    "for new_bubble, info in fusions.items():\n",
    "    if len(info['parents'])==2:\n",
    "        print(f\"Frame {info['frame']} : child {new_bubble} ← parents {info['parents']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880975e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1ab7ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusions détectées :\n",
      "Frame 2 : enfant 10 ← parents [12, 13]\n",
      "Frame 2 : enfant 12 ← parents [10, 13]\n",
      "Frame 2 : enfant 13 ← parents [11, 4]\n",
      "Frame 3 : enfant 10 ← parents [12, 13]\n",
      "Frame 3 : enfant 11 ← parents [13, 4]\n",
      "Frame 3 : enfant 12 ← parents [10, 13]\n",
      "Frame 4 : enfant 1 ← parents [14, 16]\n",
      "Frame 4 : enfant 16 ← parents [5, 17]\n",
      "Frame 4 : enfant 17 ← parents [5, 16]\n",
      "Frame 4 : enfant 10 ← parents [12, 13]\n",
      "Frame 4 : enfant 11 ← parents [13, 4]\n",
      "Frame 4 : enfant 18 ← parents [13, 11]\n",
      "Frame 4 : enfant 12 ← parents [10, 13]\n",
      "Frame 5 : enfant 1 ← parents [16, 5]\n",
      "Frame 5 : enfant 16 ← parents [5, 17]\n",
      "Frame 5 : enfant 8 ← parents [18, 11]\n",
      "Frame 5 : enfant 10 ← parents [12, 11]\n",
      "Frame 5 : enfant 11 ← parents [4, 12]\n",
      "Frame 5 : enfant 12 ← parents [10, 11]\n",
      "Frame 6 : enfant 8 ← parents [18, 19]\n",
      "Frame 6 : enfant 10 ← parents [12, 11]\n",
      "Frame 6 : enfant 18 ← parents [19, 11]\n",
      "Frame 6 : enfant 12 ← parents [10, 11]\n",
      "Frame 7 : enfant 10 ← parents [12, 11]\n",
      "Frame 7 : enfant 8 ← parents [18, 19]\n",
      "Frame 7 : enfant 11 ← parents [18, 4]\n",
      "Frame 7 : enfant 18 ← parents [19, 11]\n",
      "Frame 8 : enfant 10 ← parents [12, 11]\n",
      "Frame 8 : enfant 11 ← parents [4, 18]\n",
      "Frame 8 : enfant 20 ← parents [6, 9]\n",
      "Frame 9 : enfant 11 ← parents [4, 10]\n",
      "Frame 9 : enfant 20 ← parents [6, 9]\n",
      "Frame 13 : enfant 18 ← parents [21, 11]\n",
      "Frame 14 : enfant 18 ← parents [11, 8]\n",
      "Frame 14 : enfant 22 ← parents [18, 11]\n",
      "Frame 15 : enfant 11 ← parents [22, 18]\n",
      "Frame 15 : enfant 8 ← parents [18, 22]\n",
      "Frame 15 : enfant 23 ← parents [20, 6]\n",
      "Frame 15 : enfant 18 ← parents [22, 11]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ---------------------------\n",
    "# Paramètres (à ajuster)\n",
    "# ---------------------------\n",
    "IMAGE_SHAPE = (1024, 1024)   # (hauteur, largeur) => adapte selon tes images\n",
    "WINDOW = 2                   # regarde frames t-W ... t+W\n",
    "DILATE_ITERS = 2             # dilatation pour compenser contours imparfaits\n",
    "KERNEL = np.ones((3,3), np.uint8)\n",
    "OVERLAP_RATIO_THRESH = 0.15  # fraction de l'aire d'une bulle (ou cumulative) pour la considérer candidate\n",
    "MIN_OVERLAP_PIXELS = 50      # overlap minimal absolu (compense petites bulles)\n",
    "MAX_PARENTS = 2              # tu as dit max 2 parents\n",
    "USE_IOU = True              # si True utilise IoU en plus du ratio; sinon ratio relatif à l'aire du parent\n",
    "\n",
    "# Dossier ou sont sauvegarde les donnee apres le modele\n",
    "dataFolder = r\"C:\\Users\\faraboli\\Desktop\\BubbleID\\BubbleIDGit\\ProjetBubbleID\\My_output\\SaveData2\"\n",
    "extension = \"Test6\"\n",
    "contourFile = dataFolder + \"/contours_\" + extension +\".json\"\n",
    "richFile = dataFolder + \"/rich_\" + extension +\".csv\"\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# UTILITAIRES\n",
    "# ------------------------\n",
    "def mask_from_contour(contour, shape):\n",
    "    mask = np.zeros(shape, dtype=np.uint8)\n",
    "    if len(contour) == 0:\n",
    "        return mask\n",
    "    pts = np.array(contour, dtype=np.int32)\n",
    "    cv2.fillPoly(mask, [pts], 255)\n",
    "    if DILATE_ITERS > 0:\n",
    "        mask = cv2.dilate(mask, KERNEL, iterations=DILATE_ITERS)\n",
    "    return mask\n",
    "\n",
    "def overlap_relative_to(mask_parent, mask_child):\n",
    "    a = mask_parent > 0\n",
    "    c = mask_child > 0\n",
    "    inter = np.logical_and(a, c).sum()\n",
    "    parent_area = a.sum()\n",
    "    return (inter / parent_area if parent_area > 0 else 0.0), inter\n",
    "\n",
    "def centroid_of_mask(mask):\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(xs) == 0:\n",
    "        return None\n",
    "    return (int(np.mean(xs)), int(np.mean(ys)))  # (x, y)\n",
    "\n",
    "# ------------------------\n",
    "# CHARGEMENT\n",
    "# ------------------------\n",
    "def load_json_contours(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    parsed = {}\n",
    "    for key, contour in data.items():\n",
    "        frame_str, det_str = key.split('_')\n",
    "        frame = int(frame_str)\n",
    "        det_in_frame = int(det_str)\n",
    "        parsed[(frame, det_in_frame)] = contour\n",
    "    return parsed\n",
    "\n",
    "def build_masks_and_index(json_path, csv_path, image_shape=IMAGE_SHAPE):\n",
    "    contours = load_json_contours(json_path)\n",
    "    df = pd.read_csv(csv_path).rename(\n",
    "        columns={\"numero_image\": \"frame\", \"numero_bulle\": \"det_in_frame\"}\n",
    "    )\n",
    "\n",
    "    data_by_frame = defaultdict(dict)\n",
    "    for (frame, det_in_frame), contour in contours.items():\n",
    "        row = df[(df['frame'] == frame) & (df['det_in_frame'] == det_in_frame)]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        track_id = int(row.iloc[0]['track_id'])\n",
    "        mask = mask_from_contour(contour, image_shape)\n",
    "        data_by_frame[frame][track_id] = {\n",
    "            'mask': mask,\n",
    "            'area': np.sum(mask > 0),\n",
    "            'centroid': centroid_of_mask(mask)\n",
    "        }\n",
    "    return data_by_frame\n",
    "\n",
    "# ------------------------\n",
    "# DÉTECTION DE FUSIONS\n",
    "# ------------------------\n",
    "def detect_fusions(data_by_frame, window=WINDOW):\n",
    "    fusions = {}\n",
    "    frames = sorted(data_by_frame.keys())\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        # Ignorer la première frame (pas de parents possibles)\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        frame_data = data_by_frame[frame]\n",
    "        # Frames précédentes dans la fenêtre temporelle\n",
    "        prev_frames = [f for f in frames if f < frame and f >= frame - window]\n",
    "\n",
    "        for child_tid, child_info in frame_data.items():\n",
    "            mask_child = child_info['mask']\n",
    "            total_overlap = Counter()\n",
    "\n",
    "            # Recherche de parents dans les frames précédentes\n",
    "            for pf in prev_frames:\n",
    "                for parent_tid, parent_info in data_by_frame[pf].items():\n",
    "                    # Exclure soi-même\n",
    "                    if parent_tid == child_tid:\n",
    "                        continue\n",
    "\n",
    "                    mask_parent = parent_info['mask']\n",
    "                    ratio, inter_pix = overlap_relative_to(mask_parent, mask_child)\n",
    "\n",
    "                    if ratio > OVERLAP_RATIO_THRESH or inter_pix > MIN_OVERLAP_PIXELS:\n",
    "                        total_overlap[parent_tid] += ratio\n",
    "\n",
    "            # Garder les meilleurs parents\n",
    "            if len(total_overlap) > 0:\n",
    "                top_parents = [\n",
    "                    tid for tid, _ in total_overlap.most_common(MAX_PARENTS)\n",
    "                ]\n",
    "                # Seulement si on a ≥2 parents => vraie fusion\n",
    "                if len(top_parents) == 2:\n",
    "                    fusions[(frame, child_tid)] = top_parents\n",
    "\n",
    "    return fusions\n",
    "\n",
    "# ------------------------\n",
    "# EXÉCUTION\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "data_by_frame = build_masks_and_index(contourFile, richFile, IMAGE_SHAPE)\n",
    "fusions = detect_fusions(data_by_frame, window=2)\n",
    "\n",
    "print(\"Fusions détectées :\")\n",
    "for (frame, child), parents in fusions.items():\n",
    "    print(f\"Frame {frame} : enfant {child} ← parents {parents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7770eb0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() can't convert non-string with explicit base",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 283\u001b[0m\n\u001b[0;32m    279\u001b[0m     results_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrobust_fusion_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRésultats exportés dans \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrobust_fusion_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 283\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 263\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    261\u001b[0m richFile \u001b[38;5;241m=\u001b[39m dataFolder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/rich_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m extension \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Trouver les relations de fusion\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m fusions \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_fusion_relationships\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontourFile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrichFile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRÉSULTATS FINAUX DES FUSIONS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[40], line 140\u001b[0m, in \u001b[0;36mRobustBubbleTracker.find_fusion_relationships\u001b[1;34m(self, json_file, csv_file)\u001b[0m\n\u001b[0;32m    138\u001b[0m contours_data, track_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_data(json_file, csv_file)\n\u001b[0;32m    139\u001b[0m polygons \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_polygon_mapping(contours_data)\n\u001b[1;32m--> 140\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_track_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrack_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Trouver toutes les frames\u001b[39;00m\n\u001b[0;32m    143\u001b[0m all_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(frame \u001b[38;5;28;01mfor\u001b[39;00m frame, _ \u001b[38;5;129;01min\u001b[39;00m polygons\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "Cell \u001b[1;32mIn[40], line 38\u001b[0m, in \u001b[0;36mRobustBubbleTracker.create_track_sequences\u001b[1;34m(self, track_df)\u001b[0m\n\u001b[0;32m     36\u001b[0m tracks \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m track_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 38\u001b[0m     tracks[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_id\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mframe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdet_in_frame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Trier par frame pour chaque track\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m track_id \u001b[38;5;129;01min\u001b[39;00m tracks:\n",
      "\u001b[1;31mTypeError\u001b[0m: int() can't convert non-string with explicit base"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Set\n",
    "\n",
    "class RobustBubbleTracker:\n",
    "    def __init__(self, \n",
    "                 lookback_frames: int = 3,\n",
    "                 lookforward_frames: int = 2,\n",
    "                 iou_threshold: float = 0.4,\n",
    "                 min_disappearance_frames: int = 0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lookback_frames: Nombre de frames à regarder en arrière pour les parents\n",
    "            lookforward_frames: Nombre de frames à regarder en avant pour confirmation\n",
    "            iou_threshold: Seuil d'IoU pour considérer une relation\n",
    "            min_disappearance_frames: Nombre min de frames de disparition pour confirmer\n",
    "        \"\"\"\n",
    "        self.lookback_frames = lookback_frames\n",
    "        self.lookforward_frames = lookforward_frames\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.min_disappearance_frames = min_disappearance_frames\n",
    "        \n",
    "    def load_data(self, json_file: str, csv_file: str) -> Tuple[Dict, pd.DataFrame]:\n",
    "        \"\"\"Charge les données JSON et CSV\"\"\"\n",
    "        with open(json_file, 'r') as f:\n",
    "            contours_data = json.load(f)\n",
    "        \n",
    "        track_df = pd.read_csv(csv_file)\n",
    "        return contours_data, track_df\n",
    "    \n",
    "    def create_track_sequences(self, track_df: pd.DataFrame) -> Dict[int, List[Tuple[int, int]]]:\n",
    "        \"\"\"Crée les séquences de track_id sur les frames\"\"\"\n",
    "        tracks = defaultdict(list)\n",
    "        for _, row in track_df.iterrows():\n",
    "            tracks[row['track_id']].append(int((row['frame']), int(row['det_in_frame'])))\n",
    "        \n",
    "        # Trier par frame pour chaque track\n",
    "        for track_id in tracks:\n",
    "            tracks[track_id].sort(key=lambda x: x[0])\n",
    "        \n",
    "        return tracks\n",
    "    \n",
    "    def create_polygon_mapping(self, contours_data: dict) -> Dict[Tuple[int, int], Polygon]:\n",
    "        \"\"\"Crée le mapping (frame, det) -> Polygon\"\"\"\n",
    "        polygons = {}\n",
    "        for key, points in contours_data.items():\n",
    "            try:\n",
    "                frame_str, det_str = key.split('_')\n",
    "                frame = int(frame_str)\n",
    "                det_in_frame = int(det_str)\n",
    "                \n",
    "                if len(points) >= 3:\n",
    "                    poly = Polygon(points)\n",
    "                    if poly.is_valid:\n",
    "                        polygons[(frame, det_in_frame)] = poly\n",
    "            except:\n",
    "                continue\n",
    "        return polygons\n",
    "        \n",
    "    def find_disappeared_tracks(self, tracks: Dict[int, List[Tuple[int, int]]], \n",
    "                            current_frame: int) -> List[int]:\n",
    "        \"\"\"Trouve les tracks qui ont disparu autour de la frame courante\"\"\"\n",
    "        disappeared = []\n",
    "        \n",
    "        for track_id, sequence in tracks.items():\n",
    "            if not sequence:\n",
    "                continue\n",
    "                \n",
    "            last_frame = sequence[-1][0]\n",
    "            \n",
    "            # Le track a disparu si :\n",
    "            # 1. Sa dernière frame est dans la fenêtre [current_frame - lookback_frames, current_frame + lookforward_frames]\n",
    "            # 2. ET il n'apparaît pas APRÈS current_frame (pour éviter les tracks encore actifs)\n",
    "            \n",
    "            min_frame = current_frame - self.lookback_frames\n",
    "            max_frame = current_frame + self.lookforward_frames\n",
    "            \n",
    "            # Condition 1: dernière frame dans la fenêtre\n",
    "            if min_frame <= last_frame <= max_frame:\n",
    "                disappeared.append(track_id)\n",
    "        \n",
    "        return disappeared\n",
    "    \n",
    "    def find_new_tracks(self, tracks: Dict[int, List[Tuple[int, int]]], \n",
    "                       current_frame: int) -> List[int]:\n",
    "        \"\"\"Trouve les tracks qui sont nouvelles récemment\"\"\"\n",
    "        new_tracks = []\n",
    "        \n",
    "        for track_id, sequence in tracks.items():\n",
    "            if not sequence:\n",
    "                continue\n",
    "                \n",
    "            first_frame = sequence[0][0]\n",
    "            # Track est nouvelle si sa première frame est autour de current_frame\n",
    "            if first_frame >= current_frame and (first_frame - current_frame) <= self.lookforward_frames:\n",
    "                new_tracks.append(track_id)\n",
    "        \n",
    "        return new_tracks\n",
    "    \n",
    "    def get_track_polygons(self, track_id: int, tracks: Dict[int, List[Tuple[int, int]]],\n",
    "                          polygons: Dict[Tuple[int, int], Polygon], \n",
    "                          frame_range: Tuple[int, int]) -> List[Polygon]:\n",
    "        \"\"\"Récupère tous les polygones d'un track dans une plage de frames\"\"\"\n",
    "        track_polygons = []\n",
    "        if track_id not in tracks:\n",
    "            return track_polygons\n",
    "            \n",
    "        for frame, det_in_frame in tracks[track_id]:\n",
    "            if frame_range[0] <= frame <= frame_range[1]:\n",
    "                poly = polygons.get((frame, det_in_frame))\n",
    "                if poly and poly.is_valid:\n",
    "                    track_polygons.append(poly)\n",
    "        \n",
    "        return track_polygons\n",
    "    \n",
    "    def calculate_best_iou(self, poly1: Polygon, poly2_list: List[Polygon]) -> float:\n",
    "        \"\"\"Calcule le meilleur IoU entre un polygone et une liste de polygones\"\"\"\n",
    "        best_iou = 0.0\n",
    "        for poly2 in poly2_list:\n",
    "            if poly1.intersects(poly2):\n",
    "                try:\n",
    "                    intersection = poly1.intersection(poly2).area\n",
    "                    union = poly1.union(poly2).area\n",
    "                    iou = intersection / union if union > 0 else 0.0\n",
    "                    best_iou = max(best_iou, iou)\n",
    "                except:\n",
    "                    continue\n",
    "        return best_iou\n",
    "    \n",
    "    def find_fusion_relationships(self, json_file: str, csv_file: str) -> Dict[int, List[int]]:\n",
    "        \"\"\"\n",
    "        Trouve les relations de fusion entre bulles\n",
    "        Returns: {new_track_id: [parent_track_id1, parent_track_id2]}\n",
    "        \"\"\"\n",
    "        contours_data, track_df = self.load_data(json_file, csv_file)\n",
    "        polygons = self.create_polygon_mapping(contours_data)\n",
    "        tracks = self.create_track_sequences(track_df)\n",
    "        \n",
    "        # Trouver toutes les frames\n",
    "        all_frames = sorted(set(frame for frame, _ in polygons.keys()))\n",
    "        \n",
    "        fusion_relationships = {}\n",
    "        \n",
    "        print(\"Recherche des fusions...\")\n",
    "        \n",
    "        for current_frame in all_frames:\n",
    "            # Trouver les tracks qui ont disparu récemment (parents potentiels)\n",
    "            disappeared_tracks = self.find_disappeared_tracks(tracks, current_frame)\n",
    "            \n",
    "            # Trouver les tracks qui sont nouvelles récemment (enfants potentiels)\n",
    "            new_tracks = self.find_new_tracks(tracks, current_frame)\n",
    "            \n",
    "            if not disappeared_tracks or not new_tracks:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nFrame {current_frame}:\")\n",
    "            print(f\"  Tracks disparus: {disappeared_tracks}\")\n",
    "            print(f\"  Nouvelles tracks: {new_tracks}\")\n",
    "            \n",
    "            # Pour chaque nouvelle track, chercher ses parents parmi les tracks disparus\n",
    "            for new_track_id in new_tracks:\n",
    "                # Récupérer les polygones de la nouvelle track (sur plusieurs frames)\n",
    "                new_track_start_frame = tracks[new_track_id][0][0]\n",
    "                new_polygons = self.get_track_polygons(\n",
    "                    new_track_id, tracks, polygons,\n",
    "                    (new_track_start_frame, new_track_start_frame + self.lookforward_frames)\n",
    "                )\n",
    "                \n",
    "                if not new_polygons:\n",
    "                    continue\n",
    "                \n",
    "                potential_parents = []\n",
    "                \n",
    "                for disappeared_track_id in disappeared_tracks:\n",
    "                    # Récupérer les derniers polygones du track disparu\n",
    "                    disappeared_frames = tracks[disappeared_track_id]\n",
    "                    last_frame = disappeared_frames[-1][0]\n",
    "                    parent_polygons = self.get_track_polygons(\n",
    "                        disappeared_track_id, tracks, polygons,\n",
    "                        (max(1, last_frame - 1), last_frame)  # Dernières frames\n",
    "                    )\n",
    "                    \n",
    "                    if not parent_polygons:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculer le meilleur IoU entre les polygones\n",
    "                    best_iou = 0.0\n",
    "                    for new_poly in new_polygons:\n",
    "                        for parent_poly in parent_polygons:\n",
    "                            iou = self.calculate_best_iou(new_poly, [parent_poly])\n",
    "                            best_iou = max(best_iou, iou)\n",
    "                    \n",
    "                    if best_iou > self.iou_threshold:\n",
    "                        potential_parents.append((disappeared_track_id, best_iou))\n",
    "                        print(f\"    IoU {new_track_id}←{disappeared_track_id}: {best_iou:.3f}\")\n",
    "                \n",
    "                # Prendre les 2 meilleurs parents\n",
    "                potential_parents.sort(key=lambda x: x[1], reverse=True)\n",
    "                best_parents = [parent_id for parent_id, _ in potential_parents[:2]]\n",
    "                \n",
    "                if len(best_parents) >= 2:\n",
    "                    fusion_relationships[new_track_id] = best_parents\n",
    "                    print(f\"  FUSION DÉTECTÉE: {new_track_id} ← {best_parents}\")\n",
    "                elif len(best_parents) == 1:\n",
    "                    # On garde aussi les fusions simples pour analyse\n",
    "                    fusion_relationships[new_track_id] = best_parents\n",
    "                    print(f\"  Fusion simple: {new_track_id} ← {best_parents}\")\n",
    "        \n",
    "        return fusion_relationships\n",
    "    \n",
    "    def analyze_coexistence_frames(self, json_file: str, csv_file: str, \n",
    "                                 fusion_relationships: Dict[int, List[int]]):\n",
    "        \"\"\"Analyse les frames où parents et enfants coexistent (problèmes de segmentation)\"\"\"\n",
    "        contours_data, track_df = self.load_data(json_file, csv_file)\n",
    "        polygons = self.create_polygon_mapping(contours_data)\n",
    "        tracks = self.create_track_sequences(track_df)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ANALYSE DES COEXISTENCES\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for child_id, parent_ids in fusion_relationships.items():\n",
    "            if child_id not in tracks:\n",
    "                continue\n",
    "                \n",
    "            child_start_frame = tracks[child_id][0][0]\n",
    "            \n",
    "            for parent_id in parent_ids:\n",
    "                if parent_id not in tracks:\n",
    "                    continue\n",
    "                    \n",
    "                parent_end_frame = tracks[parent_id][-1][0]\n",
    "                \n",
    "                # Vérifier s'il y a coexistence\n",
    "                coexistence_frames = []\n",
    "                for frame in range(parent_end_frame, child_start_frame + 1):\n",
    "                    child_exists = any(f == frame for f, _ in tracks[child_id])\n",
    "                    parent_exists = any(f == frame for f, _ in tracks[parent_id])\n",
    "                    \n",
    "                    if child_exists and parent_exists:\n",
    "                        coexistence_frames.append(frame)\n",
    "                \n",
    "                if coexistence_frames:\n",
    "                    print(f\"Coexistence {parent_id} ↔ {child_id} sur frames {coexistence_frames}\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "def main():\n",
    "    tracker = RobustBubbleTracker(\n",
    "        lookback_frames=3,\n",
    "        lookforward_frames=2,\n",
    "        iou_threshold=0.4,\n",
    "        min_disappearance_frames=2\n",
    "    )\n",
    "    # Dossier ou sont sauvegarde les donnee apres le modele\n",
    "    dataFolder = r\"C:\\Users\\faraboli\\Desktop\\BubbleID\\BubbleIDGit\\ProjetBubbleID\\My_output\\SaveData2\"\n",
    "    extension = \"Test6\"\n",
    "    contourFile = dataFolder + \"/contours_\" + extension +\".json\"\n",
    "    richFile = dataFolder + \"/rich_\" + extension +\".csv\"\n",
    "    # Trouver les relations de fusion\n",
    "    fusions = tracker.find_fusion_relationships(contourFile, richFile)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RÉSULTATS FINAUX DES FUSIONS\")\n",
    "    print(\"=\"*50)\n",
    "    for child_id, parents in fusions.items():\n",
    "        print(f\"Track {child_id} est une fusion de: {parents}\")\n",
    "    \n",
    "    # Analyser les coexistences\n",
    "    tracker.analyze_coexistence_frames(contourFile, richFile, fusions)\n",
    "    \n",
    "    # Exporter les résultats\n",
    "    results = [{'child_track_id': child, 'parent_track_ids': parents} \n",
    "               for child, parents in fusions.items()]\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('robust_fusion_results.csv', index=False)\n",
    "    print(\"\\nRésultats exportés dans 'robust_fusion_results.csv'\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bubbleid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
