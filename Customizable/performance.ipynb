{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055b8b48",
   "metadata": {},
   "source": [
    "AP en utilisant 2 coco files pour le tracking performance\n",
    "Il faut creer le fichier d'annotation apres prediction sous le bon format\n",
    "Le fichier gt, doit etre bon c'est celui creer lors de la validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b602949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Selon chat pour extraire le coco apres prediction: A ADAPTER\n",
    "# outputs = model(image)  # torch tensor ou dictionnaire\n",
    "# # chaque \"output\" contient par instance :\n",
    "# # masks -> (N,H,W) booléen ou float\n",
    "# # scores -> (N,)\n",
    "# # labels -> (N,)\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import cv2\n",
    "\n",
    "# predictions_coco = []\n",
    "\n",
    "# for img_id, output in enumerate(outputs_list, start=1):\n",
    "#     masks = output['masks']  # (N,H,W)\n",
    "#     scores = output['scores']  # (N,)\n",
    "#     labels = output['labels']  # (N,)\n",
    "\n",
    "#     for mask, score, label in zip(masks, scores, labels):\n",
    "#         # binariser le masque\n",
    "#         mask_bin = (mask > 0.5).astype(np.uint8)\n",
    "#         # trouver contours\n",
    "#         contours, _ = cv2.findContours(mask_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#         for contour in contours:\n",
    "#             contour = contour.flatten().tolist()  # COCO polygone\n",
    "#             if len(contour) < 6:  # COCO exige au moins 3 points\n",
    "#                 continue\n",
    "#             x, y, w, h = cv2.boundingRect(mask_bin)\n",
    "#             predictions_coco.append({\n",
    "#                 \"image_id\": img_id,\n",
    "#                 \"category_id\": int(label),\n",
    "#                 \"segmentation\": [contour],\n",
    "#                 \"score\": float(score),\n",
    "#                 \"bbox\": [x, y, w, h]\n",
    "#             })\n",
    "\n",
    "# # Sauvegarder\n",
    "# with open(\"predictions_coco.json\", \"w\") as f:\n",
    "#     json.dump(predictions_coco, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28645f8e",
   "metadata": {},
   "source": [
    "Il faut creer le fichier coco à partir des predictions faites qui soit compatible avec le coco du validation set.\n",
    "On utilise contours_ .json et rich_ .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b9b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523\n"
     ]
    }
   ],
   "source": [
    "# On creer le coco avec les fichier deja existant rich et contour\n",
    "import json, csv\n",
    "\n",
    "extension = 'Test1'\n",
    "saveFolder = '../'\n",
    "contours_json_path = saveFolder + 'contours_' + extension + '.json'\n",
    "rich_csv_path = saveFolder + 'rich_' + extension + '.csv'\n",
    "outputCoco_path = \"predictions.json\"\n",
    "\n",
    "# Lire le fichier JSON d'entrée\n",
    "with open(contours_json_path, 'r', encoding='utf-8') as f_json:\n",
    "    contours = json.load(f_json)\n",
    "\n",
    "# Lire le fichier rich.csv et ne selectionner que les colonne qui nous interressent\n",
    "donnees_filtrees = []\n",
    "\n",
    "with open(rich_csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lecteur = csv.DictReader(f)\n",
    "    for ligne in lecteur:\n",
    "        entree = {\n",
    "            \"frame\": int(ligne[\"frame\"]),\n",
    "            \"det_in_frame\": int(ligne[\"det_in_frame\"]),\n",
    "            \"x1\": int(ligne[\"x1\"]),\n",
    "            \"y1\": int(ligne[\"y1\"]),\n",
    "            \"x2\": int(ligne[\"x2\"]),\n",
    "            \"y2\": int(ligne[\"y2\"]),\n",
    "            \"score\": float(ligne[\"score\"]),\n",
    "            \"class_id\": int(ligne[\"class_id\"])\n",
    "        }\n",
    "        donnees_filtrees.append(entree)\n",
    "\n",
    "\n",
    "if len(donnees_filtrees) != len(contours):\n",
    "    raise(\"Le fichier contours et rich n'ont pas le meme nb de lignes\")\n",
    "\n",
    "outputFile = []\n",
    "for idx, imgCode in enumerate(contours):\n",
    "    # on verifie qu'on parle de la meme image\n",
    "    i, d = map(int, imgCode.split(\"_\"))\n",
    "    if i != (donnees_filtrees[idx][\"frame\"]) or d != (donnees_filtrees[idx][\"det_in_frame\"]):\n",
    "        print(\"WARNING: les 2 fichiers ne sont pas compatibles\")\n",
    "        print(i, donnees_filtrees[idx][\"frame\"], d, donnees_filtrees[idx][\"det_in_frame\"])\n",
    "        continue\n",
    "    \n",
    "    width = (donnees_filtrees[idx][\"x2\"]) - (donnees_filtrees[idx][\"x1\"])\n",
    "    heigth = (donnees_filtrees[idx][\"y2\"]) - (donnees_filtrees[idx][\"y1\"])\n",
    "    prediction = {\n",
    "        \"image_id\": donnees_filtrees[idx][\"frame\"],\n",
    "        \"category_id\": donnees_filtrees[idx][\"class_id\"],\n",
    "        \"segmentation\": contours[imgCode],\n",
    "        \"score\": donnees_filtrees[idx][\"score\"],\n",
    "        \"bbox\": [donnees_filtrees[idx][\"x1\"], donnees_filtrees[idx][\"y1\"], width, heigth ]\n",
    "    }\n",
    "    outputFile.append(prediction)\n",
    "    \n",
    "# Écrire dans un fichier JSON\n",
    "with open(outputCoco_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(outputFile, f, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json\n",
    "\n",
    "# --- Charger les fichiers ---\n",
    "annType = 'segm'  # 'bbox' pour détection, 'segm' pour segmentation\n",
    "\n",
    "coco_gt = COCO(\"annotations_gt.json\")       # fichier COCO ground truth\n",
    "coco_dt = coco_gt.loadRes(outputCoco_path)  # fichier prédictions\n",
    "\n",
    "# --- Évaluation ---\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, annType)\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bubbleid_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
