{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055b8b48",
   "metadata": {},
   "source": [
    "AP en utilisant 2 coco files pour le tracking performance\n",
    "Il faut creer le fichier d'annotation apres prediction sous le bon format\n",
    "Le fichier gt, doit etre bon c'est celui creer lors de la validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "deb6011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the librairies\n",
    "import cv2, os\n",
    "from torch.cuda import is_available\n",
    "\n",
    "# Define the parameters for the model:\n",
    "datasetFolder = r\"C:\\Users\\faraboli\\Desktop\\BubbleID\\BubbleIDGit\\ProjetBubbleID\\training\\DATASETS\\dataset_tip_png\"\n",
    "imagesfolder = os.path.join(datasetFolder, \"val/\")   # Define the path to the folder of images\n",
    "videopath = os.path.join(datasetFolder, \"valVideo.avi\")   # Define the path to the avi video\n",
    "savefolder=\"../training/Output/\"   # Define the folder you want the data to save in\n",
    "savefolder2 = \"../training/Performances_tip/\" \n",
    "extension=\"all_jpg\"    # Define the extension you want all the saved data to have. This should be unique for each experiment\n",
    "thres=0.5    # Define the threshold for what the model identifies as a bubble\n",
    "modelweights = \"..\\\\MODELS\\\\\" + \"Models_3classes_all\" + \"\\\\model_final.pth\"\n",
    "# modelweights=r\"C:\\Users\\faraboli\\Desktop\\BubbleID\\BubbleIDGit\\ProjetBubbleID\\training\\Models_3classes_all\\model_final.pth\"     # Define the path to the saved model weights.\n",
    "# device='cpu'   # Specify if running on \"cpu\" or \"gpu\"\n",
    "device = \"cuda\" if is_available() else \"cpu\"\n",
    "print(f\"Used device : {device}\")\n",
    "\n",
    "\n",
    "validationCOCO_path = os.path.join(datasetFolder, \"val.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b2ddf",
   "metadata": {},
   "source": [
    "Si la video n'existe pas on la creer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "50918a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dossier contenant les images\n",
    "\n",
    "fichiers = sorted([f for f in os.listdir(imagesfolder) if f.endswith(\".png\")])\n",
    "N_frame = len(fichiers)\n",
    "\n",
    "\n",
    "if not os.path.exists(videopath):\n",
    "    image_exemple = cv2.imread(os.path.join(imagesfolder, fichiers[0])) # Lire la première image pour obtenir la taille\n",
    "    hauteur, largeur, _ = image_exemple.shape\n",
    "\n",
    "    # Créer l'objet vidéo\n",
    "    fps = 5  # images par seconde\n",
    "    videopath = os.path.join(datasetFolder, \"valVideo.avi\")\n",
    "    codec = cv2.VideoWriter_fourcc(*'XVID')  # ou 'XVID' pour .avi\n",
    "    video = cv2.VideoWriter(videopath, codec, fps, (largeur, hauteur))\n",
    "\n",
    "    # Ajouter chaque image à la vidéo\n",
    "    for fichier in fichiers:\n",
    "        chemin = os.path.join(imagesfolder, fichier)\n",
    "        image = cv2.imread(chemin)\n",
    "        video.write(image)\n",
    "\n",
    "    # Finaliser\n",
    "    video.release()\n",
    "    print(f\"Vidéo créée : {videopath}\")\n",
    "\n",
    "imagesfolder = os.path.join(datasetFolder, \"trim/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab383f",
   "metadata": {},
   "source": [
    "# Prediction sur le validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "945d4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import BubbleID_My\n",
    "\n",
    "# recharge le module pour prendre en compte les modifications\n",
    "BubbleID_My = importlib.reload(BubbleID_My)\n",
    "\n",
    "import BubbleID_My as BubbleID\n",
    "print(BubbleID.__file__)\n",
    "\n",
    "# Instantiating the class\n",
    "test120=BubbleID.DataAnalysis(imagesfolder,videopath,savefolder,extension,modelweights,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4d964371",
   "metadata": {},
   "outputs": [],
   "source": [
    "test120.trimVideo(N_frames_extr=N_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "43edba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data from the images:\n",
    "#test120.set_scale_by_two_points(frame_idx=0, physical_mm=20.0)\n",
    "mm = test120.set_scale_by_two_points(frame_idx=0, physical_mm=20.0, save=True)\n",
    "print(\"mm/px =\", mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "42140252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-22 19:00:08] INFO - checkpoint.py - [Checkpointer] Loading from ..\\MODELS\\Models_3classes_all\\model_final.pth ...\n",
      "100%|██████████| 56/56 [00:06<00:00,  8.59it/s]\n"
     ]
    }
   ],
   "source": [
    "test120.GenerateData(thres, save_rich=True, save_masks=False, save_contours=True, iou_thresh_tid=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28645f8e",
   "metadata": {},
   "source": [
    "# Creation du coco prediction\n",
    "Il faut creer le fichier coco à partir des predictions faites qui soit compatible avec le coco du validation set.\n",
    "On utilise contours_ .json et rich_ .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "536b9b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On creer le coco avec les fichier deja existant rich et contour\n",
    "import json, csv\n",
    "\n",
    "\n",
    "contours_json_path = savefolder + 'contours_' + extension + '.json'\n",
    "rich_csv_path = savefolder + 'rich_' + extension + '.csv'\n",
    "outputCoco_path = os.path.join(savefolder2,\"predictions_\" + extension + \".json\")\n",
    "\n",
    "# Lire le fichier JSON d'entrée\n",
    "with open(contours_json_path, 'r', encoding='utf-8') as f_json:\n",
    "    contours = json.load(f_json)\n",
    "\n",
    "# Lire le fichier rich.csv et ne selectionner que les colonne qui nous interressent\n",
    "donnees_filtrees = []\n",
    "\n",
    "with open(rich_csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lecteur = csv.DictReader(f)\n",
    "    for ligne in lecteur:\n",
    "        entree = {\n",
    "            \"frame\": int(ligne[\"frame\"]),\n",
    "            \"det_in_frame\": int(ligne[\"det_in_frame\"]),\n",
    "            \"x1\": int(ligne[\"x1\"]),\n",
    "            \"y1\": int(ligne[\"y1\"]),\n",
    "            \"x2\": int(ligne[\"x2\"]),\n",
    "            \"y2\": int(ligne[\"y2\"]),\n",
    "            \"score\": float(ligne[\"score\"]),\n",
    "            \"class_id\": int(ligne[\"class_id\"])\n",
    "        }\n",
    "        donnees_filtrees.append(entree)\n",
    "\n",
    "\n",
    "if len(donnees_filtrees) != len(contours):\n",
    "    raise(\"Le fichier contours et rich n'ont pas le meme nb de lignes\")\n",
    "\n",
    "outputFile = []\n",
    "for idx, imgCode in enumerate(contours):\n",
    "    # on verifie qu'on parle de la meme image\n",
    "    i, d = map(int, imgCode.split(\"_\"))\n",
    "    if i != (donnees_filtrees[idx][\"frame\"]) or d != (donnees_filtrees[idx][\"det_in_frame\"]):\n",
    "        print(\"WARNING: les 2 fichiers ne sont pas compatibles\")\n",
    "        print(i, donnees_filtrees[idx][\"frame\"], d, donnees_filtrees[idx][\"det_in_frame\"])\n",
    "        continue\n",
    "    \n",
    "    width = (donnees_filtrees[idx][\"x2\"]) - (donnees_filtrees[idx][\"x1\"])\n",
    "    heigth = (donnees_filtrees[idx][\"y2\"]) - (donnees_filtrees[idx][\"y1\"])\n",
    "    flat_coords = [x for pt in contours[imgCode] for x in pt]\n",
    "    prediction = {\n",
    "        \"image_id\": donnees_filtrees[idx][\"frame\"] - 1 , #Dans ce fichier les idx commencent a 1 alors que dans l'autre ils commencent a 0\n",
    "        \"category_id\": donnees_filtrees[idx][\"class_id\"],\n",
    "        \"segmentation\": [flat_coords],\n",
    "        \"score\": donnees_filtrees[idx][\"score\"],\n",
    "        \"bbox\": [donnees_filtrees[idx][\"x1\"], donnees_filtrees[idx][\"y1\"], width, heigth ]\n",
    "    }\n",
    "    outputFile.append(prediction)\n",
    "os.makedirs(savefolder2, exist_ok=True)   \n",
    "# Écrire dans un fichier JSON\n",
    "with open(outputCoco_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(outputFile, f, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3bfed8",
   "metadata": {},
   "source": [
    "# Evaluation des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8705fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# --- Charger les fichiers ---\n",
    "annType = 'segm'  # 'bbox' pour détection, 'segm' pour segmentation\n",
    "\n",
    "coco_gt = COCO(validationCOCO_path)       # fichier COCO ground truth\n",
    "coco_dt = coco_gt.loadRes(outputCoco_path)  # fichier prédictions\n",
    "\n",
    "# --- Évaluation ---\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, annType)\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "# --- Capture de la sortie de summarize ---\n",
    "buffer = io.StringIO()\n",
    "sys.stdout = buffer  # Redirige stdout vers le buffer\n",
    "coco_eval.summarize()\n",
    "sys.stdout = sys.__stdout__  # Restaure stdout\n",
    "\n",
    "# --- Sauvegarde dans un fichier texte ---\n",
    "outputResult_path = os.path.join(savefolder2,\"performance_\" + extension + \".json\")\n",
    "\n",
    "with open(outputResult_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a5147",
   "metadata": {},
   "source": [
    "# Resume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5d4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric\n",
      "IoU\n",
      "Area\n",
      "maxDets\n",
      "all\n",
      "all_jpg\n",
      "cav\n",
      "old\n",
      "tip\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('Metric', 'IoU', 'Area', 'old', 'all', 'all_jpeg', 'cav', 'tip', 'alldiff', 'all_jpgdiff', 'cavdiff', 'tipdiff')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\afara\\AppData\\Local\\miniconda3\\envs\\bubbleid_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ('Metric', 'IoU', 'Area', 'old', 'all', 'all_jpeg', 'cav', 'tip', 'alldiff', 'all_jpgdiff', 'cavdiff', 'tipdiff')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     tableau[\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiff\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (tableau[\u001b[38;5;28mid\u001b[39m]\u001b[38;5;241m-\u001b[39mtableau[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m/\u001b[39mtableau[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 29\u001b[0m tableau \u001b[38;5;241m=\u001b[39m \u001b[43mtableau\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMetric\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIoU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mArea\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall_jpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malldiff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall_jpgdiff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcavdiff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtipdiff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# tableau.to_csv(\"../training/performanceResumeAll.csv\", index=False)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\afara\\AppData\\Local\\miniconda3\\envs\\bubbleid_env\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\afara\\AppData\\Local\\miniconda3\\envs\\bubbleid_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: ('Metric', 'IoU', 'Area', 'old', 'all', 'all_jpeg', 'cav', 'tip', 'alldiff', 'all_jpgdiff', 'cavdiff', 'tipdiff')"
     ]
    }
   ],
   "source": [
    "IoU = [\"0.50:0.95\" , \"0.50\",\"0.75\"  ,\"0.50:0.95\" ,\"0.50:0.95\" ,\"0.50:0.95\" , \"0.50:0.95\" , \"0.50:0.95\" ,\" 0.50:0.95\" ,\"0.50:0.95\" , \"0.50:0.95\" , \"0.50:0.95\"] \n",
    "Metric = [\"AP\", \"AP\", \"AP\", \"AP\", \"AP\", \"AP\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\"]\n",
    "Area = [\"all\", \"all\", \"all\", \"small\", \"medium\", \"large\", \"all\", \"all\", \"all\", \"small\", \"medium\", \"large\"]\n",
    "maxDets = [100, 100, 100, 100, 100, 100, 1, 10, 100, 100, 100, 100]\n",
    "tableau = pd.DataFrame({\"Metric\":Metric, \"IoU\":IoU, \"Area\": Area, \"maxDets\":maxDets})\n",
    "\n",
    "\n",
    "dossier = r\"C:\\Users\\afara\\Documents\\EPFL\\cours\\MA3\\Projet\\ProjetBubbleID\\training\\Performances_all\"\n",
    "fichiers = sorted([f for f in os.listdir(dossier) if f.endswith(\".txt\")])\n",
    "\n",
    "#  Lecture de chaque fichier\n",
    "for fichier in fichiers:\n",
    "    chemin = os.path.join(dossier, fichier)\n",
    "    identifiant = os.path.splitext(fichier)[0][12:]  \n",
    "\n",
    "    with open(chemin, \"r\", encoding=\"utf-8\") as f:\n",
    "        lignes = f.readlines()\n",
    "    valeurs = []\n",
    "    for ligne in lignes:\n",
    "        valeurs.append(float(ligne[-6:-1]))\n",
    "    tableau[identifiant] = valeurs\n",
    "    \n",
    "# for id in tableau:\n",
    "#     print(id)\n",
    "#     if id in [\"old\", \"Area\", \"maxDets\", \"IoU\", \"Metric\"]:\n",
    "#         continue\n",
    "#     tableau[id+\"diff\"] = (tableau[id]-tableau[\"old\"])/tableau[\"old\"]\n",
    "    \n",
    "tableau #= tableau[\"Metric\", \"IoU\", \"Area\", \"old\", \"all\", \"all_jpeg\", \"cav\", \"tip\", \"alldiff\", \"all_jpgdiff\", \"cavdiff\", \"tipdiff\"]\n",
    "tableau.to_csv(\"../training/performanceResumeAll.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bc7691",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bubbleid_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
